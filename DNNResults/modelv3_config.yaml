activation: relu
batch_size: 512
dropout_prob: 0.1
hidden_layers: 2
hidden_neurons: 32
learning_rate: 0.0001
num_epochs: 30
