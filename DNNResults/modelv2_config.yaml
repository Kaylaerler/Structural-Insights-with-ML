activation: relu
batch_size: 1024
learning_rate: 0.001
neurons_per_layer:
- 50
num_epochs: 30
