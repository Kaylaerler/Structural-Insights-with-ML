activation: relu
batch_size: 512
dropout_prob: 0.1
hidden_layers: 1
hidden_neurons: 32
learning_rate: 0.001
num_epochs: 100
